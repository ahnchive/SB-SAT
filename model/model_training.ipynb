{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model_training.ipynb","provenance":[],"authorship_tag":"ABX9TyObqzU+8XMr3/XJapxFJJQi"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"m2skL6tBIK5V","colab_type":"code","outputId":"50352239-1bda-4034-a906-7df8bd7f0c9a","executionInfo":{"status":"ok","timestamp":1580777823272,"user_tz":300,"elapsed":18030,"user":{"displayName":"Seoyoung Ahn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUxCxn88bLBvv8HWbbKJFSlNnCd3eIHF-iBqUg4Q=s64","userId":"17379736260487986874"}},"colab":{"base_uri":"https://localhost:8080/","height":128}},"source":["## mount gdrive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GJ7Zxqq9ggwc","colab_type":"code","outputId":"a401073b-b9fe-4e14-d047-86251ce44c52","executionInfo":{"status":"ok","timestamp":1580777823386,"user_tz":300,"elapsed":17589,"user":{"displayName":"Seoyoung Ahn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUxCxn88bLBvv8HWbbKJFSlNnCd3eIHF-iBqUg4Q=s64","userId":"17379736260487986874"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["## go to your working directory\n","cd '/content/gdrive/My Drive/project/18SAT/script'"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/project/18SAT/script\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1yung2S4gjTe","colab_type":"code","outputId":"334809b8-1cc6-4a1a-aa2a-00d60ba5c709","executionInfo":{"status":"ok","timestamp":1580777827514,"user_tz":300,"elapsed":2559,"user":{"displayName":"Seoyoung Ahn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUxCxn88bLBvv8HWbbKJFSlNnCd3eIHF-iBqUg4Q=s64","userId":"17379736260487986874"}},"colab":{"base_uri":"https://localhost:8080/","height":100}},"source":["import os\n","import pandas as pd\n","import numpy as np\n","import json\n","import h5py\n","import pathlib\n","import pickle\n","from collections import defaultdict\n","import random\n","from copy import deepcopy\n","from datetime import datetime\n","\n","from sklearn.utils import shuffle, class_weight\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.metrics import balanced_accuracy_score\n","import itertools\n","\n","from keras.models import model_from_json\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.utils import np_utils\n","from keras.models import Sequential, Model\n","from keras.layers import Input, Dense, Dropout, Activation, Flatten\n","from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, LSTM, GRU, RNN, CuDNNGRU, CuDNNLSTM, Bidirectional\n","from keras.layers.normalization import BatchNormalization\n","from keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta\n","from keras import backend as K\n","from keras import regularizers\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from IPython.core.debugger import set_trace"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"ywPp6qaggtMT","colab_type":"code","colab":{}},"source":["## global variable\n","datanorm = True\n","datacols = ['CURRENT_FIX_X', 'CURRENT_FIX_Y', 'CURRENT_FIX_PUPIL', 'CURRENT_FIX_DURATION']\n","\n","delta = 10 # size of window based on the middle point\n","step =  2*delta+1 #if you want no overlap: 2*delta+1 # size of step in window extraction\n","\n","datasplit = 'subject'\n","\n","labelcols = ['subj', 'book',\n","            'acc_level', 'subj_acc_level', \n","            'confidence', 'difficulty', 'familiarity', 'recognition', \n","            'interest', 'pressured', 'sleepiness', 'sleephours',\n","            'sex', 'native']\n","\n","pred_variable = 'subj_acc_level'\n","\n","modeltype = 'cnn'\n","\n","BATCH_SIZE = 100\n","EPOCHS = 1000\n","\n","# data path\n","file_fixdata = '../data/18sat_fixfinal.csv'\n","file_trialdata = '../data/18sat_trialfinal.csv'\n","file_labels = '../data/18sat_labels.csv'\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mD5coREi6JU_","colab_type":"code","outputId":"5023b2f4-937a-4282-99e8-53276b3886d9","executionInfo":{"status":"ok","timestamp":1580777834789,"user_tz":300,"elapsed":696,"user":{"displayName":"Seoyoung Ahn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUxCxn88bLBvv8HWbbKJFSlNnCd3eIHF-iBqUg4Q=s64","userId":"17379736260487986874"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["## score file load\n","sc = pd.read_csv(file_labels)\n","\n","## select label columns\n","sc = sc[labelcols]\n","\n","## cut/replace values\n","sc['sex'] = sc['sex'].replace(['F', 'M'], [1,0])\n","binarycols = ('recognition', 'sex', 'native')\n","subsetcols = [c for c in labelcols if c not in binarycols]\n","sc[subsetcols] = sc[subsetcols].replace([0,1,2,3], [0,0,1,1])\n","\n","## frequency table per column\n","for column in sc:\n","    print(sc[column].value_counts(sort=False, dropna=False), '\\n')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["msd034    4\n","msd063    4\n","msd019    4\n","msd089    4\n","msd086    4\n","         ..\n","msd054    4\n","msd055    4\n","msd085    4\n","msd070    4\n","msd101    4\n","Name: subj, Length: 95, dtype: int64 \n","\n","northpole    95\n","genome       95\n","dickens      95\n","flytrap      95\n","Name: book, dtype: int64 \n","\n","0    149\n","1    231\n","Name: acc_level, dtype: int64 \n","\n","0    200\n","1    180\n","Name: subj_acc_level, dtype: int64 \n","\n","0    219\n","1    161\n","Name: confidence, dtype: int64 \n","\n","0    252\n","1    128\n","Name: difficulty, dtype: int64 \n","\n","0    291\n","1     89\n","Name: familiarity, dtype: int64 \n","\n","0    373\n","1      7\n","Name: recognition, dtype: int64 \n","\n","0    150\n","1    230\n","Name: interest, dtype: int64 \n","\n","0    250\n","1    130\n","Name: pressured, dtype: int64 \n","\n","0    233\n","1    147\n","Name: sleepiness, dtype: int64 \n","\n","0    180\n","1    200\n","Name: sleephours, dtype: int64 \n","\n","0    128\n","1    252\n","Name: sex, dtype: int64 \n","\n","0    116\n","1    264\n","Name: native, dtype: int64 \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eeSwjeWyAwM8","colab_type":"text"},"source":["## create dataset"]},{"cell_type":"code","metadata":{"id":"wYeAm06DHwkI","colab_type":"code","colab":{}},"source":["## preprocessing to window\n","def group_windows(fixationRows):\n","    windows = []\n","    fixationRows.reset_index(inplace=True)\n","    for n in range(delta, len(fixationRows)-delta, step):\n","        window = fixationRows.loc[n-delta:n+delta,datacols].values.tolist()\n","        windows.append(window)\n","    return windows\n","\n","# original version\n","    # nRow = fixationRows.shape[0]\n","    # nCol = fixationRows.shape[1]\n","    # windows = []\n","\n","    # fixNum = 0\n","\n","    # for index, row in fixationRows.iterrows():\n","    #     if (index+1)%\n","    #     if fixNum + delta <= nRow-1 and fixNum - delta >= 0:\n","    #         deltaMin = fixNum - delta\n","    #         deltaMax = fixNum + delta\n","    #         window = []\n","    #         for i in range(deltaMin, deltaMax+1):\n","    #             x = fixationRows['CURRENT_FIX_X']\n","    #             x = x.values[i]\n","    #             y = fixationRows['CURRENT_FIX_Y'] # - yOffset\n","    #             y = y.values[i]\n","    #             d = fixationRows['CURRENT_FIX_DURATION']\n","    #             d = d.values[i]\n","    #             p = fixationRows['CURRENT_FIX_PUPIL']\n","    #             p = p.values[i]\n","    #             #r = fixationRows['CURRENT_FIX_INTEREST_AREA_LABEL']\n","    #             #r = r.values[i]\n","    #             window.append([x, y, d, p])\n","    #         windows.append(window)\n","    #     fixNum += 1\n","    # return windows\n","\n","## Loop over all articles and subjects\n","def generate_windata(fixation):\n","    subjectPool = pd.unique(fixation['RECORDING_SESSION_LABEL'])\n","    pagePool = pd.unique(fixation['page_name'])\n","    windowData = {}\n","    for subject in subjectPool:\n","        subjectRows = fixation.loc[fixation['RECORDING_SESSION_LABEL'] == subject]\n","        windowData[subject] = {}\n","        print(\"\\rprocessing Subject: \" + subject, end='')\n","        for page in pagePool:\n","            # print (\"Subject: \" + subject + \", Page: \" + page)\n","            pageRows = subjectRows.loc[subjectRows['page_name'] == page]\n","            # visualize_article(article, subjectRows)\n","            windows = group_windows(pageRows)\n","            windowData[subject][page] = windows\n","    print (\"\\nwindow data ready\")\n","    return windowData\n","\n","\n","## create dataset \n","def create_dataset(windowData, sc):\n","    dataset = []\n","    index= []\n","    labeldf = pd.DataFrame()\n","\n","    for subject in windowData:\n","        for article in windowData[subject]:\n","            windows = windowData[subject][article]\n","            for window in windows:\n","                dataset.append(window)\n","                book = article.split('-')[1] # article = 'reading-dickens-1'\n","                row = sc[(sc['subj'] == subject) & (sc['book'] == book)]\n","                labeldf = pd.concat([labeldf, row]) \n","    \n","    print('dataset created')\n","    return np.array(dataset), labeldf  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pPPrP5y8nNYx","colab_type":"code","outputId":"d1f2ca40-c8c3-43df-dba9-8b136eee5151","executionInfo":{"status":"ok","timestamp":1580777866151,"user_tz":300,"elapsed":24016,"user":{"displayName":"Seoyoung Ahn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUxCxn88bLBvv8HWbbKJFSlNnCd3eIHF-iBqUg4Q=s64","userId":"17379736260487986874"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["## fixation data load\n","fd = pd.read_csv(file_fixdata) #encoding = \"ISO-8859-1\",\\\n","\n","## data split\n","fd_rd = fd.loc[fd.type == 'reading']\n","# fd_comp = fd.loc[(fd.type == 'question') & (fd.page <= 5)]\n","# fd_mental = fd.loc[(fd.type == 'question') & (fd.page > 5)]\n","\n","## data normalization\n","if datanorm:\n","    # normalized_df=(df-df.mean())/df.std()\n","    # normalized_df=(df-df.min())/(df.max()-df.min())\n","    # fd_rd_mean = fd_rd.copy(deep=True)\n","    # fd_rd_mean[cols]=(fd_rd[cols]-fd_rd[cols].mean())/fd_rd[cols].std()\n","    fixData = fd_rd.copy(deep=True)\n","    fixData[datacols] = (fd_rd[datacols]-fd_rd[datacols].min())/(fd_rd[datacols].max()-fd_rd[datacols].min())\n","else:\n","    fixData = fd_rd.copy(deep=True)\n","\n","fixData[datacols].describe()\n","\n","# from sklearn import preprocessing\n","\n","# x = df.values #returns a numpy array\n","# min_max_scaler = preprocessing.MinMaxScaler()\n","# x_scaled = min_max_scaler.fit_transform(x)\n","# df_scaled = pd.DataFrame(x_scaled)\n","\n","## call function\n","windowData = generate_windata(fixData)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["processing Subject: msd107\n","window data ready\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0pvcbHWns6qX","colab_type":"code","colab":{}},"source":["## save windowdata\n","with open('../data/windowData_'+str(delta)+'.pkl', 'wb') as fp:\n","    pickle.dump(windowData, fp)\n","\n","# ## load windowdata\n","# with open('../data/windowData_'+str(delta)+'.pkl', 'rb') as fp:\n","#     windowData = pickle.load(fp)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4YRDwEnisBYz","colab_type":"code","outputId":"9f1ad38a-473e-48b4-fdc3-b00ad0245a56","executionInfo":{"status":"ok","timestamp":1579831106961,"user_tz":-540,"elapsed":24230,"user":{"displayName":"Seoyoung Ahn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUxCxn88bLBvv8HWbbKJFSlNnCd3eIHF-iBqUg4Q=s64","userId":"17379736260487986874"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["## data split\n","datasplit = 'book'\n","\n","if datasplit == 'subject':\n","    # subject-wise dataset split  (subject wise)\n","    # current plan is to use 60:20:20 \n","    subjkeys = list(windowData.keys())\n","    random.Random(23).shuffle(subjkeys) #random shuffling\n","    N_totalsub = len(subjkeys)\n","    N_trainsub = round(0.6*N_totalsub)\n","    N_validsub = round(0.2*N_totalsub)\n","    N_testsub = N_totalsub - N_trainsub - N_validsub\n","\n","    windowData_train = deepcopy(windowData)\n","    windowData_valid = {}\n","    windowData_test = {}\n","\n","    for i, subj in enumerate(subjkeys):\n","        if i in range(N_validsub):\n","            #print(subj, 'to valid')\n","            windowData_valid[subj] = windowData_train[subj]\n","            del windowData_train[subj]\n","        elif i in range(N_validsub, N_validsub + N_testsub):\n","            #print(subj, 'to test')\n","            windowData_test[subj] = windowData_train[subj]\n","            del windowData_train[subj]\n","\n","    print(\"train subj #\", len(list(windowData_train.keys())))\n","    print(\"valid subj #\", len(list(windowData_valid.keys())))\n","    print(\"test subj #\", len(list(windowData_test.keys())))\n","\n","    ## create dataset\n","    X_train, labels_train = create_dataset(windowData_train, sc)\n","    X_valid, labels_valid = create_dataset(windowData_valid, sc)\n","    X_test, labels_test = create_dataset(windowData_test, sc)\n","        \n","elif datasplit == 'record':\n","    X, labels = create_dataset(windowData, sc)\n","    X_train, X_test, labels_train, labels_test = train_test_split(X, labels, test_size=0.4, random_state=23)\n","    X_valid, X_test, labels_valid, labels_test = train_test_split(X_test, labels_test, test_size=0.5, random_state=23)\n","\n","elif datasplit == 'book':\n","    # book-wise dataset split  \n","    ## current plan is to use 50:25:25 (2,1,1)\n","\n","    subjkeys = list(windowData.keys())\n","    pagekeys = list(windowData[subjkeys[0]].keys())\n","    bookkeys = list(np.unique(sc['book'])) # ['dickens' 'flytrap' 'genome' 'northpole']\n","    print('list of books:', bookkeys)\n","\n","    windowData_train = deepcopy(windowData)\n","    windowData_valid = defaultdict(dict)\n","    windowData_test = defaultdict(dict)\n","\n","    for subj in subjkeys:\n","        tmp = random.sample(bookkeys,2)\n","        for page in pagekeys:\n","            if (page.split('-')[1] == tmp[0]):\n","                windowData_valid[subj][page] = windowData_train[subj][page]\n","                del windowData_train[subj][page]\n","                \n","            elif (page.split('-')[1] == tmp[1]): \n","                windowData_test[subj][page] = windowData_train[subj][page]\n","                del windowData_train[subj][page]\n","\n","    ## create dataset\n","    X_train, labels_train = create_dataset(windowData_train, sc)\n","    X_valid, labels_valid = create_dataset(windowData_valid, sc)\n","    X_test, labels_test = create_dataset(windowData_test, sc)\n","\n","    print(\"train book #\", list(windowData_train['msd001'].keys()))\n","    print(\"valid book #\", list(windowData_valid['msd001'].keys()))\n","    print(\"test book #\", list(windowData_test['msd001'].keys()))\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["list of books: ['dickens', 'flytrap', 'genome', 'northpole']\n","dataset created\n","dataset created\n","dataset created\n","train book # ['reading-dickens-1', 'reading-dickens-2', 'reading-dickens-3', 'reading-dickens-4', 'reading-dickens-5', 'reading-flytrap-1', 'reading-flytrap-2', 'reading-flytrap-3', 'reading-flytrap-4', 'reading-flytrap-5', 'reading-flytrap-6']\n","valid book # ['reading-genome-1', 'reading-genome-2', 'reading-genome-3', 'reading-genome-4', 'reading-genome-5', 'reading-genome-6']\n","test book # ['reading-northpole-1', 'reading-northpole-2', 'reading-northpole-3', 'reading-northpole-4', 'reading-northpole-5']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lFqqF4cgQnsE","colab_type":"code","colab":{}},"source":["## save dataset\n","np.save('../dataset/'+ datasplit +'wise/train/fix_train_' + str(delta) + '.npy', X_train)\n","np.save('../dataset/' + datasplit + 'wise/val/fix_valid_' + str(delta) + '.npy', X_valid)\n","np.save('../dataset/'+ datasplit + 'wise/test/fix_test_' + str(delta) + '.npy', X_test)\n","\n","labels_train.to_csv('../dataset/'+ datasplit +'wise/train/label_train_' + str(delta)+ '.csv', index=False)\n","labels_valid.to_csv('../dataset/'+ datasplit +'wise/val/label_train_' + str(delta) + '.csv', index=False)\n","labels_test.to_csv('../dataset/'+ datasplit + 'wise/test/label_train_' + str(delta) + '.csv', index=False)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0TbWUpc3UeTq","colab_type":"text"},"source":["## model training"]},{"cell_type":"code","metadata":{"id":"tlF02JZMUil9","colab_type":"code","colab":{}},"source":["## load dataset\n","X_train = np.load('../dataset/'+ datasplit +'wise/train/fix_train_' + str(delta) + '.npy')\n","X_valid = np.load('../dataset/' + datasplit + 'wise/val/fix_valid_' + str(delta) + '.npy')\n","X_test = np.load('../dataset/'+ datasplit + 'wise/test/fix_test_' + str(delta) + '.npy')\n","\n","labels_train = pd.read_csv('../dataset/'+ datasplit +'wise/train/label_train_' + str(delta)+ '.csv')\n","labels_valid = pd.read_csv('../dataset/'+ datasplit +'wise/val/label_train_' + str(delta) + '.csv')\n","labels_test = pd.read_csv('../dataset/'+ datasplit + 'wise/test/label_train_' + str(delta) + '.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xBT8yscEHH2X","colab_type":"code","outputId":"e49b9c7d-8782-4f55-c37d-09e33dd88d8e","executionInfo":{"status":"ok","timestamp":1580621137415,"user_tz":300,"elapsed":6186,"user":{"displayName":"Seoyoung Ahn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUxCxn88bLBvv8HWbbKJFSlNnCd3eIHF-iBqUg4Q=s64","userId":"17379736260487986874"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["## create dataset\n","pred_variable = 'difficulty'\n","\n","if pred_variable == 'subj':\n","    ## labels as categorical\n","    y_train = labels_train[pred_variable].astype('category').cat.codes\n","    y_valid = labels_valid[pred_variable].astype('category').cat.codes\n","    y_test = labels_test[pred_variable].astype('category').cat.codes\n","\n","else:\n","    ## labels as categorical\n","    y_train = labels_train[pred_variable]\n","    y_valid = labels_valid[pred_variable]\n","    y_test = labels_test[pred_variable]\n","\n","# ## randomize row for training data\n","# from sklearn.utils import shuffle\n","# X_train, labels_train, idx_train = shuffle(X_train, labels_train, idx_train, random_state=23)\n","\n","## data description \n","num_classes = len(pd.unique(y_train)) # labels_train[pred_variable].shape (TTTT,)\n","\n","print(\"##### data description #####\")\n","print(\"# of classes:\\t\",num_classes)\n","\n","input_shape = X_train.shape[1:]\n","print(\"input shape is:\\t\",input_shape)\n","\n","N_samples_train = X_train.shape[0]\n","print(\"# of samples for training is:\\t\", N_samples_train)\n","\n","N_samples_valid = X_valid.shape[0]\n","print(\"# of samples for validation is:\\t\", N_samples_valid)\n","\n","N_samples_test = X_test.shape[0]\n","print(\"# of samples for prediction is:\\t\", N_samples_test)\n","\n","N_total = N_samples_train + N_samples_valid + N_samples_test\n","print(\"# of total sampels:\\t\", N_total)\n","\n","## check data imbalances and caculate weights for loss\n","weights = class_weight.compute_class_weight('balanced'\n","        ,np.unique(y_train)\n","        ,y_train)\n","\n","print(\"\\n##### data imbalances #####\")\n","print(y_train.value_counts(normalize=True).sort_index())\n","\n","print(\"\\n##### loss weight #####\")\n","weights = dict(enumerate(weights))\n","print(weights)\n","\n","print(\"\\n##### null acc for test dataset #####\")\n","print(np.max(y_test.value_counts(normalize=True).sort_index()))\n","\n","## one hot encoding\n","y_train = np_utils.to_categorical(y_train, num_classes)\n","y_valid = np_utils.to_categorical(y_valid, num_classes)\n","y_test = np_utils.to_categorical(y_test, num_classes)\n","\n","\n","# ##### data description #####\n","# # of classes:\t 4\n","# input shape is:\t (41, 4)\n","# # of samples for training is:\t 3216\n","# # of samples for validation is:\t 1109\n","# # of samples for prediction is:\t 1106\n","# # of total sampels:\t 5431\n","\n","# ##### data imbalances #####\n","# 0    0.152674\n","# 1    0.255908\n","# 2    0.317475\n","# 3    0.273943\n","# Name: acc_level, dtype: float64\n","\n","# ##### loss weight #####\n","# {0: 1.6374745417515275, 1: 0.976913730255164, 2: 0.7874632713026445, 3: 0.9125993189557321}\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["##### data description #####\n","# of classes:\t 2\n","input shape is:\t (21, 4)\n","# of samples for training is:\t 6867\n","# of samples for validation is:\t 2362\n","# of samples for prediction is:\t 2319\n","# of total sampels:\t 11548\n","\n","##### data imbalances #####\n","0    0.615116\n","1    0.384884\n","Name: difficulty, dtype: float64\n","\n","##### loss weight #####\n","{0: 0.8128551136363636, 1: 1.2990919409761634}\n","\n","##### null acc for test dataset #####\n","0.7654161276412247\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6DRXiGEXI1LI","colab_type":"code","outputId":"8a5cedea-8817-42a5-b2c4-40dc17caa917","executionInfo":{"status":"ok","timestamp":1579848809588,"user_tz":-540,"elapsed":2132,"user":{"displayName":"Seoyoung Ahn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUxCxn88bLBvv8HWbbKJFSlNnCd3eIHF-iBqUg4Q=s64","userId":"17379736260487986874"}},"colab":{"base_uri":"https://localhost:8080/","height":476}},"source":["## model specify and compile\n","modeltype = 'rnn'\n","\n","if 'model' in globals():\n","    del model\n","    K.clear_session()\n","\n","if modeltype == 'linear':\n","    inputs = Input(shape=input_shape)\n","    x = Flatten()(inputs)\n","\n","    # x = Dense(100)(x)\n","    # # x = BatchNormalization()(x)\n","    # x = Activation('relu')(x)\n","\n","    # x = Dense(100)(x)\n","    # # x = BatchNormalization()(x)\n","    # x = Activation('relu')(x)\n","\n","    # x = Dense(100)(x)\n","    # # x = BatchNormalization()(x)\n","    # x = Activation('relu')(x)\n","\n","\n","    predictions = Dense(num_classes, activation='softmax')(x)\n","    model = Model(inputs=inputs, outputs=predictions)\n","    model.summary()\n","\n","elif modeltype == 'cnn':\n","\n","    ## 62 bookwise, 64 recordwise for subj_acc_level  (dense 50, 10)\n","    model = Sequential()\n","\n","    model.add(Conv1D(40, 3, input_shape= input_shape)) #padding= 'same', #use_bias = False\n","    # model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    # model.add(BatchNormalization())\n","\n","    model.add(Conv1D(40, 3)) #padding= 'same', #use_bias = False\n","    # model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    # model.add(BatchNormalization())\n","\n","    model.add(Conv1D(40, 3)) #padding= 'same', #use_bias = False\n","    # model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    # model.add(BatchNormalization())\n","\n","    # model.add(Conv1D(20, 3)) #padding= 'same', #use_bias = False\n","    # # model.add(BatchNormalization())\n","    # model.add(Activation('relu'))\n","    # # model.add(BatchNormalization())\n","\n","    model.add(MaxPooling1D(2))\n","    model.add(Dropout(0.3))\n","\n","    model.add(Flatten())\n","\n","    model.add(Dense(50)) # kernel_regularizer=regularizers.l2(reg)\n","    # model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    #model.add(BatchNormalization())\n","    model.add(Dropout(0.3))\n","\n","    model.add(Dense(20)) # kernel_regularizer=regularizers.l2(reg)\n","    # model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    #model.add(BatchNormalization())\n","\n","    model.add(Dense(num_classes, activation='softmax'))\n","\n","    print(model.summary())\n","\n","\n","\n","    ## 0422 96% acc\n","    # model = Sequential()\n","\n","    # model.add(Conv1D(64, 3, padding= 'same', input_shape= input_shape)) #use_bias = False\n","    # model.add(BatchNormalization())\n","    # model.add(Activation('relu'))\n","\n","    # model.add(Conv1D(64, 3, padding= 'same'))\n","    # model.add(BatchNormalization())\n","    # model.add(Activation('relu'))\n","\n","    # model.add(Conv1D(64, 3))\n","    # model.add(BatchNormalization())\n","    # model.add(Activation('relu'))\n","\n","    # # model.add(Conv1D(64, 3))\n","    # # model.add(BatchNormalization())\n","    # # model.add(Activation('relu'))\n","\n","    # model.add(Flatten())\n","    # model.add(Dropout(0.5))\n","\n","    # model.add(Dense(20))\n","    # model.add(BatchNormalization())\n","    # model.add(Activation('relu'))\n","    # # model.add(Dropout(0.5))\n","\n","    # model.add(Dense(num_classes, activation='softmax'))\n","    \n","    # print(model.summary())\n","\n","\n","elif modeltype == 'rnn':\n","    model = Sequential()\n","\n","    # model.add(BatchNormalization(input_shape=input_shape))\n","    model.add(Bidirectional(LSTM(25, return_sequences = True),input_shape=input_shape)) \n","    # model.add(BatchNormalization())\n","    # model.add(Bidirectional(CuDNNLSTM(32, return_sequences = True)))\n","    # model.add(BatchNormalization())\n","    model.add(Bidirectional(LSTM(25)))\n","    model.add(Dropout(0.3))\n","\n","    model.add(Dense(50)) # kernel_regularizer=regularizers.l2(reg)\n","    # model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    #model.add(BatchNormalization())\n","    model.add(Dropout(0.3))\n","\n","    model.add(Dense(20)) # kernel_regularizer=regularizers.l2(reg)\n","    # model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    #model.add(BatchNormalization())\n","\n","    model.add(Dense(num_classes, activation='softmax'))\n","    print(model.summary())\n","\n","    # model.add(BatchNormalization(input_shape=input_shape))\n","\n","    # #model.add(LSTM(4, input_shape=input_shape, activation='relu'))\n","    # #model.add(LSTM(4, input_shape=input_shape, activation='relu', return_sequences = True))\n","    # #model.add(LSTM(4, activation='relu', return_sequences = True))\n","    # #model.add(LSTM(4, activation='relu', return_sequences = True))\n","    # #model.add(LSTM(4, activation='relu', return_sequences = True))\n","    # #model.add(LSTM(4, activation='relu'))\n","\n","    # # model.add(CuDNNLSTM(4, return_sequences = True))\n","    # # model.add(CuDNNLSTM(4, return_sequences = True))\n","    # # #model.add(LSTM(4, activation='relu', return_sequences = True))\n","    # # model.add(CuDNNLSTM(4))\n","\n","    # # model.add(CuDNNGRU(4, return_sequences = True))\n","    # # model.add(CuDNNGRU(4, return_sequences = True))\n","    # # #model.add(LSTM(4, activation='relu', return_sequences = True))\n","    # # model.add(CuDNNGRU(4))\n","\n","    # #model.add(Bidirectional(CuDNNLSTM(32)))\n","    # model.add(Bidirectional(CuDNNLSTM(32, return_sequences = True)))\n","    # model.add(BatchNormalization())\n","\n","    # model.add(Bidirectional(CuDNNLSTM(32, return_sequences = True)))\n","    # model.add(BatchNormalization())\n","\n","    # # #model.add(LSTM(4, activation='relu', return_sequences = True))\n","    # model.add(Bidirectional(CuDNNLSTM(32)))\n","    # model.add(BatchNormalization())\n","    # model.add(Dropout(0.1))\n","\n","    # #model.add(Dropout(0.2))\n","\n","    # # model.add(Dense(10))\n","    # # model.add(BatchNormalization())\n","    # # model.add(Activation('relu'))\n","\n","    # model.add(Dense(num_classes, activation='softmax'))\n","    # model.summary()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","bidirectional_1 (Bidirection (None, 21, 50)            6000      \n","_________________________________________________________________\n","bidirectional_2 (Bidirection (None, 50)                15200     \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 50)                0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 50)                2550      \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 50)                0         \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 50)                0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 20)                1020      \n","_________________________________________________________________\n","activation_2 (Activation)    (None, 20)                0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 2)                 42        \n","=================================================================\n","Total params: 24,812\n","Trainable params: 24,812\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ycn_g_aEK1j4","colab_type":"code","outputId":"37106f62-d23e-4d59-c82c-1b79a7044a00","executionInfo":{"status":"ok","timestamp":1579849082863,"user_tz":-540,"elapsed":266924,"user":{"displayName":"Seoyoung Ahn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUxCxn88bLBvv8HWbbKJFSlNnCd3eIHF-iBqUg4Q=s64","userId":"17379736260487986874"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# BATCH_SIZE = 100\n","# EPOCHS = 1000\n","\n","#learning_rate = 0.0001\n","#decay_rate= 1e-04\n","#optimizer = RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-08, decay=decay_rate) \n","#optimizer = SGD(lr=learning_rate, decay=decay_rate, momentum=0.9, nesterov=True)\n","#optimizer = Adagrad(lr=learning_rate, epsilon=None, decay=decay_rate)\n","#optimizer = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=decay_rate, amsgrad=False)\n","#optimizer = Adadelta()\n","\n","#model.compile(loss='categorical_crossentropy', optimizer= optimizer, metrics=['accuracy'])\n","model.compile(loss='categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n","\n","## checkpoint path\n","date_time = datetime.now()\n","d = date_time.strftime(\"%m%d%H%M\")\n","print(d)\n","\n","callbacks_list = [\n","    ModelCheckpoint(\n","        filepath= '../checkpoint/' + str(d) +'-{epoch:02d}-{val_acc:.2f}.h5',\n","        monitor='val_acc', save_best_only=True),\n","    EarlyStopping(monitor='val_loss', patience=50)\n","]\n","\n","hist = model.fit(X_train, y_train, batch_size=BATCH_SIZE,  epochs =EPOCHS,  \n","                 class_weight = weights, verbose=2, callbacks=callbacks_list,\n","                 validation_data= (X_valid, y_valid), shuffle=True) \n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["01240653\n","Train on 6867 samples, validate on 2362 samples\n","Epoch 1/1000\n"," - 7s - loss: 0.6938 - acc: 0.5203 - val_loss: 0.7007 - val_acc: 0.3323\n","Epoch 2/1000\n"," - 5s - loss: 0.6923 - acc: 0.4963 - val_loss: 0.6849 - val_acc: 0.6677\n","Epoch 3/1000\n"," - 5s - loss: 0.6922 - acc: 0.4873 - val_loss: 0.6908 - val_acc: 0.4890\n","Epoch 4/1000\n"," - 5s - loss: 0.6856 - acc: 0.5368 - val_loss: 0.6949 - val_acc: 0.5440\n","Epoch 5/1000\n"," - 5s - loss: 0.6787 - acc: 0.5592 - val_loss: 0.6909 - val_acc: 0.5864\n","Epoch 6/1000\n"," - 5s - loss: 0.6743 - acc: 0.5707 - val_loss: 0.6570 - val_acc: 0.6710\n","Epoch 7/1000\n"," - 5s - loss: 0.6684 - acc: 0.5864 - val_loss: 0.6905 - val_acc: 0.5449\n","Epoch 8/1000\n"," - 5s - loss: 0.6567 - acc: 0.6014 - val_loss: 0.6903 - val_acc: 0.5576\n","Epoch 9/1000\n"," - 5s - loss: 0.6663 - acc: 0.5796 - val_loss: 0.7251 - val_acc: 0.4568\n","Epoch 10/1000\n"," - 4s - loss: 0.6608 - acc: 0.5991 - val_loss: 0.6973 - val_acc: 0.5178\n","Epoch 11/1000\n"," - 5s - loss: 0.6487 - acc: 0.6131 - val_loss: 0.6879 - val_acc: 0.5390\n","Epoch 12/1000\n"," - 5s - loss: 0.6465 - acc: 0.6122 - val_loss: 0.6753 - val_acc: 0.5830\n","Epoch 13/1000\n"," - 4s - loss: 0.6417 - acc: 0.6279 - val_loss: 0.6844 - val_acc: 0.5563\n","Epoch 14/1000\n"," - 5s - loss: 0.6399 - acc: 0.6370 - val_loss: 0.7093 - val_acc: 0.5224\n","Epoch 15/1000\n"," - 5s - loss: 0.6387 - acc: 0.6285 - val_loss: 0.6925 - val_acc: 0.5762\n","Epoch 16/1000\n"," - 5s - loss: 0.6343 - acc: 0.6375 - val_loss: 0.6953 - val_acc: 0.5428\n","Epoch 17/1000\n"," - 5s - loss: 0.6251 - acc: 0.6512 - val_loss: 0.7429 - val_acc: 0.5351\n","Epoch 18/1000\n"," - 5s - loss: 0.6288 - acc: 0.6472 - val_loss: 0.6842 - val_acc: 0.5754\n","Epoch 19/1000\n"," - 5s - loss: 0.6194 - acc: 0.6619 - val_loss: 0.7016 - val_acc: 0.5398\n","Epoch 20/1000\n"," - 5s - loss: 0.6239 - acc: 0.6549 - val_loss: 0.6966 - val_acc: 0.5673\n","Epoch 21/1000\n"," - 4s - loss: 0.6198 - acc: 0.6674 - val_loss: 0.7371 - val_acc: 0.4716\n","Epoch 22/1000\n"," - 5s - loss: 0.6190 - acc: 0.6587 - val_loss: 0.7428 - val_acc: 0.4746\n","Epoch 23/1000\n"," - 5s - loss: 0.6118 - acc: 0.6667 - val_loss: 0.7282 - val_acc: 0.5440\n","Epoch 24/1000\n"," - 5s - loss: 0.6094 - acc: 0.6652 - val_loss: 0.7511 - val_acc: 0.5068\n","Epoch 25/1000\n"," - 4s - loss: 0.6051 - acc: 0.6680 - val_loss: 0.7322 - val_acc: 0.5318\n","Epoch 26/1000\n"," - 4s - loss: 0.6043 - acc: 0.6719 - val_loss: 0.7223 - val_acc: 0.5500\n","Epoch 27/1000\n"," - 4s - loss: 0.6091 - acc: 0.6696 - val_loss: 0.7153 - val_acc: 0.5517\n","Epoch 28/1000\n"," - 5s - loss: 0.6040 - acc: 0.6713 - val_loss: 0.6948 - val_acc: 0.5792\n","Epoch 29/1000\n"," - 5s - loss: 0.5968 - acc: 0.6770 - val_loss: 0.7388 - val_acc: 0.5131\n","Epoch 30/1000\n"," - 5s - loss: 0.5972 - acc: 0.6763 - val_loss: 0.7520 - val_acc: 0.5165\n","Epoch 31/1000\n"," - 4s - loss: 0.6098 - acc: 0.6617 - val_loss: 0.7429 - val_acc: 0.5262\n","Epoch 32/1000\n"," - 5s - loss: 0.5923 - acc: 0.6872 - val_loss: 0.7228 - val_acc: 0.5279\n","Epoch 33/1000\n"," - 5s - loss: 0.5959 - acc: 0.6763 - val_loss: 0.7269 - val_acc: 0.5326\n","Epoch 34/1000\n"," - 4s - loss: 0.5967 - acc: 0.6764 - val_loss: 0.7205 - val_acc: 0.5419\n","Epoch 35/1000\n"," - 5s - loss: 0.6080 - acc: 0.6614 - val_loss: 0.7256 - val_acc: 0.5466\n","Epoch 36/1000\n"," - 5s - loss: 0.6011 - acc: 0.6763 - val_loss: 0.7275 - val_acc: 0.5191\n","Epoch 37/1000\n"," - 5s - loss: 0.5923 - acc: 0.6884 - val_loss: 0.7286 - val_acc: 0.5411\n","Epoch 38/1000\n"," - 4s - loss: 0.5889 - acc: 0.6891 - val_loss: 0.7133 - val_acc: 0.5652\n","Epoch 39/1000\n"," - 4s - loss: 0.5910 - acc: 0.6849 - val_loss: 0.7033 - val_acc: 0.5745\n","Epoch 40/1000\n"," - 5s - loss: 0.5892 - acc: 0.6850 - val_loss: 0.7450 - val_acc: 0.5491\n","Epoch 41/1000\n"," - 5s - loss: 0.5970 - acc: 0.6753 - val_loss: 0.7345 - val_acc: 0.5330\n","Epoch 42/1000\n"," - 5s - loss: 0.5908 - acc: 0.6868 - val_loss: 0.7370 - val_acc: 0.5233\n","Epoch 43/1000\n"," - 5s - loss: 0.5893 - acc: 0.6809 - val_loss: 0.7536 - val_acc: 0.5047\n","Epoch 44/1000\n"," - 5s - loss: 0.5941 - acc: 0.6822 - val_loss: 0.6863 - val_acc: 0.5919\n","Epoch 45/1000\n"," - 5s - loss: 0.5883 - acc: 0.6879 - val_loss: 0.7394 - val_acc: 0.5140\n","Epoch 46/1000\n"," - 4s - loss: 0.5862 - acc: 0.6849 - val_loss: 0.7158 - val_acc: 0.5559\n","Epoch 47/1000\n"," - 4s - loss: 0.5880 - acc: 0.6872 - val_loss: 0.7216 - val_acc: 0.5487\n","Epoch 48/1000\n"," - 5s - loss: 0.5876 - acc: 0.6853 - val_loss: 0.7532 - val_acc: 0.5042\n","Epoch 49/1000\n"," - 5s - loss: 0.5876 - acc: 0.6875 - val_loss: 0.7548 - val_acc: 0.5038\n","Epoch 50/1000\n"," - 5s - loss: 0.5848 - acc: 0.6841 - val_loss: 0.7203 - val_acc: 0.5377\n","Epoch 51/1000\n"," - 5s - loss: 0.5853 - acc: 0.6863 - val_loss: 0.7333 - val_acc: 0.5237\n","Epoch 52/1000\n"," - 5s - loss: 0.5751 - acc: 0.6961 - val_loss: 0.7305 - val_acc: 0.5466\n","Epoch 53/1000\n"," - 5s - loss: 0.5811 - acc: 0.6898 - val_loss: 0.7305 - val_acc: 0.5288\n","Epoch 54/1000\n"," - 4s - loss: 0.5801 - acc: 0.6885 - val_loss: 0.7381 - val_acc: 0.5178\n","Epoch 55/1000\n"," - 5s - loss: 0.5849 - acc: 0.6888 - val_loss: 0.7425 - val_acc: 0.4958\n","Epoch 56/1000\n"," - 5s - loss: 0.5779 - acc: 0.6892 - val_loss: 0.7205 - val_acc: 0.5343\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wvXoO0uan4qn","colab_type":"code","colab":{}},"source":["## visualizing losses and accuracy\n","\n","## figures path\n","fig_loss = path_result +'/figure/rnn0430_norm_loss_to'  + str(datasize) + '_w'+ str(delta) + '.png'\n","fig_acc =  path_result +'/figure/rnn0430_norm_acc_to'  + str(datasize) + '_w'+ str(delta) + '.png'\n","\n","\n","train_loss = hist.history['loss']\n","val_loss = hist.history['val_loss']\n","train_acc = hist.history['acc']\n","val_acc = hist.history['val_acc']\n","xc = range(201) #EPOCHS run or see whether early stoppings\n","\n","# train_loss =  hist.history['loss'] + hist2.history['loss']\n","# val_loss = hist.history['val_loss'] + hist2.history['val_loss']\n","# train_acc = hist.history['acc'] + hist2.history['acc']\n","# val_acc = hist.history['val_acc'] + hist2.history['val_acc']\n","# xc = range(2*EPOCHS) #EPOCHS run or see whether early stoppings\n","\n","\n","plt.figure(1, figsize=(7, 5), facecolor=\"white\")\n","plt.plot(xc, train_loss)\n","plt.plot(xc, val_loss)\n","plt.xlabel('num of Epochs')\n","plt.ylabel('loss')\n","plt.title('train_loss vs val_loss')\n","plt.grid(True)\n","plt.legend(['train', 'val'])\n","# print plt.style.available # use bmh, classic,ggplot for big pictures\n","plt.style.use(['classic'])\n","plt.savefig(fig_loss)\n","\n","plt.figure(2, figsize=(7, 5),facecolor=\"white\")\n","plt.plot(xc, train_acc)\n","plt.plot(xc, val_acc)\n","plt.xlabel('num of Epochs')\n","plt.ylabel('accuracy')\n","plt.title('train_acc vs val_acc')\n","plt.grid(True)\n","plt.legend(['train', 'val']) #, loc=4\n","# print plt.style.available # use bmh, classic,ggplot for big pictures\n","plt.style.use(['classic'])\n","plt.savefig(fig_acc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7MjNfTxXnqHK","colab_type":"text"},"source":["## evaluate the model"]},{"cell_type":"code","metadata":{"id":"fbGlMcrgoHOi","colab_type":"code","colab":{}},"source":["datasplit = 'subject'\n","\n","## load dataset\n","X_train = np.load('../dataset/'+ datasplit +'wise/train/fix_train_' + str(delta) + '.npy')\n","X_valid = np.load('../dataset/' + datasplit + 'wise/val/fix_valid_' + str(delta) + '.npy')\n","X_test = np.load('../dataset/'+ datasplit + 'wise/test/fix_test_' + str(delta) + '.npy')\n","\n","labels_train = pd.read_csv('../dataset/'+ datasplit +'wise/train/label_train_' + str(delta)+ '.csv')\n","labels_valid = pd.read_csv('../dataset/'+ datasplit +'wise/val/label_train_' + str(delta) + '.csv')\n","labels_test = pd.read_csv('../dataset/'+ datasplit + 'wise/test/label_train_' + str(delta) + '.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qdf_DMWZq8vM","colab_type":"code","outputId":"6e795e76-c502-4ecd-f5ce-50829e696a00","executionInfo":{"status":"ok","timestamp":1579863104193,"user_tz":-540,"elapsed":856,"user":{"displayName":"Seoyoung Ahn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUxCxn88bLBvv8HWbbKJFSlNnCd3eIHF-iBqUg4Q=s64","userId":"17379736260487986874"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["pred_variable = 'native'\n","\n","## create dataset\n","if pred_variable == 'subj':\n","    ## labels as categorical\n","    y_train = labels_train[pred_variable].astype('category').cat.codes\n","    y_valid = labels_valid[pred_variable].astype('category').cat.codes\n","    y_test = labels_test[pred_variable].astype('category').cat.codes\n","\n","else:\n","    ## labels as categorical\n","    y_train = labels_train[pred_variable]\n","    y_valid = labels_valid[pred_variable]\n","    y_test = labels_test[pred_variable]\n","\n","# ## randomize row for training data\n","# from sklearn.utils import shuffle\n","# X_train, labels_train, idx_train = shuffle(X_train, labels_train, idx_train, random_state=23)\n","\n","## data description \n","num_classes = len(pd.unique(y_train)) # labels_train[pred_variable].shape (TTTT,)\n","\n","print(\"##### data description #####\")\n","print(\"# of classes:\\t\",num_classes)\n","\n","input_shape = X_train.shape[1:]\n","print(\"input shape is:\\t\",input_shape)\n","\n","N_samples_train = X_train.shape[0]\n","print(\"# of samples for training is:\\t\", N_samples_train)\n","\n","N_samples_valid = X_valid.shape[0]\n","print(\"# of samples for validation is:\\t\", N_samples_valid)\n","\n","N_samples_test = X_test.shape[0]\n","print(\"# of samples for prediction is:\\t\", N_samples_test)\n","\n","N_total = N_samples_train + N_samples_valid + N_samples_test\n","print(\"# of total sampels:\\t\", N_total)\n","\n","## check data imbalances and caculate weights for loss\n","weights = class_weight.compute_class_weight('balanced'\n","        ,np.unique(y_train)\n","        ,y_train)\n","\n","print(\"\\n##### data imbalances #####\")\n","print(y_train.value_counts(normalize=True).sort_index())\n","\n","print(\"\\n##### loss weight #####\")\n","weights = dict(enumerate(weights))\n","print(weights)\n","\n","print(\"\\n##### null acc for test dataset #####\")\n","print(np.max(y_test.value_counts(normalize=True).sort_index()))\n","\n","## one hot encoding\n","y_train = np_utils.to_categorical(y_train, num_classes)\n","y_valid = np_utils.to_categorical(y_valid, num_classes)\n","y_test = np_utils.to_categorical(y_test, num_classes)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["##### data description #####\n","# of classes:\t 2\n","input shape is:\t (21, 4)\n","# of samples for training is:\t 6867\n","# of samples for validation is:\t 2362\n","# of samples for prediction is:\t 2319\n","# of total sampels:\t 11548\n","\n","##### data imbalances #####\n","0    0.385612\n","1    0.614388\n","Name: native, dtype: float64\n","\n","##### loss weight #####\n","{0: 1.2966389728096677, 1: 0.8138184403887178}\n","\n","##### null acc for test dataset #####\n","0.7705907718844329\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rmOOxtDMuajH","colab_type":"code","outputId":"daa10e63-fd5d-4099-e624-e3e1eeede711","executionInfo":{"status":"ok","timestamp":1579863246204,"user_tz":-540,"elapsed":32548,"user":{"displayName":"Seoyoung Ahn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUxCxn88bLBvv8HWbbKJFSlNnCd3eIHF-iBqUg4Q=s64","userId":"17379736260487986874"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["## load model\n","from keras.models import load_model\n","\n","modeltype = 'rnn'\n","\n","file_model = '../savedmodel/'+ pred_variable + '/' + datasplit+ 'wise/' + modeltype + '.h5'\n","loaded_model = load_model(file_model)\n","print(\"Loaded model from disk\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loaded model from disk\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_-TMdgh1LQtc","colab_type":"code","outputId":"2617c479-cc03-49cf-fc56-6ae990f40ad7","executionInfo":{"status":"ok","timestamp":1579863257876,"user_tz":-540,"elapsed":43575,"user":{"displayName":"Seoyoung Ahn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUxCxn88bLBvv8HWbbKJFSlNnCd3eIHF-iBqUg4Q=s64","userId":"17379736260487986874"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["## Evaluating the model\n","print(\"predicted variable:\", pred_variable, '\\n')\n","\n","score = loaded_model.evaluate(X_test, y_test, verbose=0)\n","print('Test Loss:', score[0])\n","print('Test accuracy:', score[1])\n","\n","# Printing the confusion matrix\n","\n","Y_pred = loaded_model.predict(X_test)\n","y_pred = np.argmax(Y_pred, axis=1)\n","#y_pred = model.predict_classes(X_test)\n","# target_names = ['F', 'M']\n","# target_names = ['sleepy', 'awake']\n","#target_names = ['hard', 'easy']\n","\n","target_names = ['level {}'.format(i) for i in range(num_classes)]\n","\n","print(classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names))\n","print(\"confusion matrix: \\n\", confusion_matrix(np.argmax(y_test, axis=1), y_pred))\n","\n","# cm = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n","# bas = (0.5*cm[0,0])/(cm[0,0]+cm[0,1]) + (0.5*cm[1,1])/(cm[1,0]+cm[1,1])\n","# print (\"Balanced acc score:\",bas )\n","# print (\"Balanced error rate:\",1 - bas)\n","\n","print(\"Balanced acc score:\", balanced_accuracy_score(np.argmax(y_test, axis=1), y_pred))\n","print(\"Balanced error rate:\", 1- balanced_accuracy_score(np.argmax(y_test, axis=1), y_pred))\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["predicted variable: native \n","\n","Test Loss: 0.649511418706541\n","Test accuracy: 0.7447175507133713\n","              precision    recall  f1-score   support\n","\n","     level 0       0.39      0.20      0.27       532\n","     level 1       0.79      0.91      0.85      1787\n","\n","    accuracy                           0.74      2319\n","   macro avg       0.59      0.55      0.56      2319\n","weighted avg       0.70      0.74      0.71      2319\n","\n","confusion matrix: \n"," [[ 108  424]\n"," [ 168 1619]]\n","Balanced acc score: 0.5544976038305052\n","Balanced error rate: 0.44550239616949483\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CPs20taSL8me","colab_type":"code","colab":{}},"source":["## confusion matrix\n","sns.set_style(\"white\")\n","cfmtrix = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n","\n","df_cm = pd.DataFrame(cfmtrix, index = [i for i in target_names],\n","                  columns = [i for i in target_names])\n","plt.figure(figsize = (10,7))\n","sns.heatmap(df_cm, annot=True,fmt=\"d\")"],"execution_count":0,"outputs":[]}]}